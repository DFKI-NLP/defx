{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/deft_split/subtask1_raw/train_dev/'\n",
    "test_path = '../data/deft_split/subtask1_raw/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subtask1_file(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with file_path.open() as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quotechar='\"')\n",
    "        for row in reader:\n",
    "            sentences.append(row[0])\n",
    "            labels.append(int(row[1]))\n",
    "    return sentences, labels\n",
    "    \n",
    "def read_subtask1_corpus(file_path):\n",
    "    corpus = []\n",
    "    labels = []\n",
    "    input_path = Path(file_path)\n",
    "    if input_path.is_dir():\n",
    "        for file_name in Path(file_path).iterdir():\n",
    "            sentences, sent_labels = read_subtask1_file(file_name)\n",
    "            corpus.extend(sentences)\n",
    "            labels.extend(sent_labels)\n",
    "    else:\n",
    "        print('loading file')\n",
    "        corpus, labels = read_subtask1_file(input_path)\n",
    "    return corpus, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, Y_train = read_subtask1_corpus(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16659"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus, Y_test = read_subtask1_corpus(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_setup(pipeline):\n",
    "    pipeline.fit(corpus, Y_train)\n",
    "    Y_test_pred = pipeline.predict(test_corpus)\n",
    "    print(metrics.classification_report(Y_test, Y_test_pred))\n",
    "    print(pd.DataFrame(metrics.confusion_matrix(Y_test, Y_test_pred)))\n",
    "    return Y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline =  Pipeline([\n",
    "    ('vect', CountVectorizer(lowercase=False, ngram_range=(1,3))),\n",
    "    ('tidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       537\n",
      "           1       0.71      0.71      0.71       273\n",
      "\n",
      "    accuracy                           0.80       810\n",
      "   macro avg       0.78      0.78      0.78       810\n",
      "weighted avg       0.80      0.80      0.80       810\n",
      "\n",
      "     0    1\n",
      "0  459   78\n",
      "1   80  193\n"
     ]
    }
   ],
   "source": [
    "Y_test_pred = evaluate_setup(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write model and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_input_path = '../data/deft_split/subtask1_raw/test/'\n",
    "results_path = '../data/results/subtask1__svm_baseline__dev/'\n",
    "Path(results_path).mkdir(exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in Path(predictions_input_path).iterdir():\n",
    "    text, _ = read_subtask1_file(input_file)\n",
    "    pred_Y = pipeline.predict(text)\n",
    "    output_file_name = \"task_1_\" + input_file.name\n",
    "    output_file = os.path.join(results_path, output_file_name)\n",
    "    with open(output_file, 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "        for prediction in zip(text, pred_Y):\n",
    "            writer.writerow(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(Path(results_path, \"task_1_svm_submission.zip\"), 'w') as zf:\n",
    "    for pred_file in Path(results_path).iterdir():\n",
    "        if pred_file.suffix == '.deft':\n",
    "            zf.write(pred_file, pred_file.name)\n",
    "            #pred_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = os.path.join(results_path, 'svm_pipeline.pickle')\n",
    "with open(model_file, 'wb') as f:\n",
    "    pickle.dump(pipeline, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model deserialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       537\n",
      "           1       0.71      0.71      0.71       273\n",
      "\n",
      "    accuracy                           0.80       810\n",
      "   macro avg       0.78      0.78      0.78       810\n",
      "weighted avg       0.80      0.80      0.80       810\n",
      "\n",
      "     0    1\n",
      "0  459   78\n",
      "1   80  193\n"
     ]
    }
   ],
   "source": [
    "with open(model_file, 'rb') as f:\n",
    "    loaded_pipeline = pickle.load(f)\n",
    "evaluate_setup(loaded_pipeline);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
